{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21dd21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sep 17 15:09:38 2018\n",
    "\n",
    "@author: HAN_RUIZHI yb77447@umac.mo OR  501248792@qq.com\n",
    "\n",
    "This code is the first version of BLS Python. \n",
    "If you have any questions about the code or find any bugs\n",
    "   or errors during use, please feel free to contact me.\n",
    "If you have any questions about the original paper, \n",
    "   please contact the authors of related paper.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from numpy import random\n",
    "from scipy import linalg as LA \n",
    "import time\n",
    "\n",
    "\n",
    "def show_accuracy(predictLabel, Label): \n",
    "    count = 0\n",
    "    label_1 = np.zeros(Label.shape[0])\n",
    "    predlabel = []\n",
    "    label_1 = Label.argmax(axis=1)\n",
    "    predlabel = predictLabel.argmax(axis=1)\n",
    "    for j in list(range(Label.shape[0])):\n",
    "        if label_1[j] == predlabel[j]:\n",
    "            count += 1\n",
    "    return (round(count/len(Label),5))\n",
    "\n",
    "\n",
    "def tansig(x):\n",
    "    return (2/(1+np.exp(-2*x)))-1\n",
    "\n",
    "\n",
    "def sigmoid(data):\n",
    "    return 1.0/(1+np.exp(-data))\n",
    "    \n",
    "\n",
    "def linear(data):\n",
    "    return data\n",
    "    \n",
    "\n",
    "def tanh(data):\n",
    "    return (np.exp(data)-np.exp(-data))/(np.exp(data)+np.exp(-data))\n",
    "    \n",
    "\n",
    "def relu(data):\n",
    "    return np.maximum(data, 0)\n",
    "\n",
    "# 广义逆矩阵\n",
    "def pinv(A, reg):\n",
    "    return np.mat(reg*np.eye(A.shape[1])+A.T.dot(A)).I.dot(A.T)\n",
    "\n",
    "# 稀疏表示\n",
    "# a是输入的数据，b表示阈值，用于控制稀疏化的程度\n",
    "def shrinkage(a, b):\n",
    "    z = np.maximum(a - b, 0) - np.maximum( -a - b, 0)\n",
    "    return z\n",
    "\n",
    "\n",
    "def sparse_bls(A, b):\n",
    "    lam = 0.001\n",
    "    itrs = 50\n",
    "    # ATA\n",
    "    # A是特征矩阵，m为样本数，n为特征数\n",
    "    AA = A.T.dot(A)   \n",
    "    #数据的个数\n",
    "    m = A.shape[1]\n",
    "    n = b.shape[1]\n",
    "    #生成m维n列的0向量\n",
    "    x1 = np.zeros([m, n])\n",
    "    wk = x1\n",
    "    ok = x1\n",
    "    uk = x1\n",
    "    # np.mat创建的始终是二维的对象数组matrix，输入是一维数组或者是多维，也会强制转换为二维\n",
    "    # np.eye生成的是二维矩阵且对角线元素是1\n",
    "    # 生成m行m列的对角矩阵\n",
    "    # L1就是(A^TA+I)^(-1)\n",
    "    L1 = np.mat(AA + np.eye(m)).I\n",
    "    L2 = (L1.dot(A.T)).dot(b)\n",
    "    for i in range(itrs):\n",
    "        ck = L2 + np.dot(L1, (ok - uk))\n",
    "        ok = shrinkage(ck + uk, lam)\n",
    "        uk = uk + ck - ok\n",
    "        wk = ok\n",
    "    return wk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82cb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "def BLS(train_x, train_y, test_x, test_y, s, c, N1, N2, N3):\n",
    "    L = 0\n",
    "    train_x = preprocessing.scale(train_x, axis=1)\n",
    "    FeatureOfInputDataWithBias = np.hstack([train_x, 0.1 * np.ones((train_x.shape[0],1))])\n",
    "    OutputOfFeatureMappingLayer = np.zeros([train_x.shape[0], N2*N1])\n",
    "    Beta1OfEachWindow = []\n",
    "\n",
    "    distOfMaxAndMin = []\n",
    "    minOfEachWindow = []\n",
    "    ymin = 0\n",
    "    ymax = 1\n",
    "    train_acc_all = np.zeros([1,L+1])\n",
    "    test_acc = np.zeros([1,L+1])\n",
    "    train_time = np.zeros([1,L+1])\n",
    "    test_time = np.zeros([1,L+1])\n",
    "    time_start=time.time()#计时开始\n",
    "    for i in range(N2):\n",
    "        random.seed(i)\n",
    "        weightOfEachWindow = 2 * random.randn(train_x.shape[1]+1,N1)-1\n",
    "        FeatureOfEachWindow = np.dot(FeatureOfInputDataWithBias, weightOfEachWindow) \n",
    "        scaler1 = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(FeatureOfEachWindow)\n",
    "        FeatureOfEachWindowAfterPreprocess = scaler1.transform(FeatureOfEachWindow)\n",
    "        betaOfEachWindow  =  sparse_bls(FeatureOfEachWindowAfterPreprocess,FeatureOfInputDataWithBias).T\n",
    "        Beta1OfEachWindow.append(betaOfEachWindow)\n",
    "        outputOfEachWindow = np.dot(FeatureOfInputDataWithBias,betaOfEachWindow)\n",
    "#        print('Feature nodes in window: max:',np.max(outputOfEachWindow),'min:',np.min(outputOfEachWindow))\n",
    "        distOfMaxAndMin.append(np.max(outputOfEachWindow,axis =0) - np.min(outputOfEachWindow,axis=0))\n",
    "        minOfEachWindow.append(np.min(outputOfEachWindow,axis = 0))\n",
    "        outputOfEachWindow = (outputOfEachWindow-minOfEachWindow[i])/distOfMaxAndMin[i]\n",
    "        OutputOfFeatureMappingLayer[:, N1*i:N1*(i+1)] = outputOfEachWindow\n",
    "        del outputOfEachWindow \n",
    "        del FeatureOfEachWindow \n",
    "        del weightOfEachWindow \n",
    "\n",
    "    InputOfEnhanceLayerWithBias = np.hstack([OutputOfFeatureMappingLayer, 0.1 * np.ones((OutputOfFeatureMappingLayer.shape[0],1))])\n",
    "\n",
    "    if N1*N2>=N3:\n",
    "        random.seed(67797325)\n",
    "        weightOfEnhanceLayer = LA.orth(2 * random.randn(N2*N1+1,N3))-1\n",
    "    else:\n",
    "        random.seed(67797325)\n",
    "        weightOfEnhanceLayer = LA.orth(2 * random.randn(N2*N1+1,N3).T-1).T\n",
    "    \n",
    "    tempOfOutputOfEnhanceLayer = np.dot(InputOfEnhanceLayerWithBias,weightOfEnhanceLayer)\n",
    "#    print('Enhance nodes: max:',np.max(tempOfOutputOfEnhanceLayer),'min:',np.min(tempOfOutputOfEnhanceLayer))\n",
    "\n",
    "    parameterOfShrink = s/np.max(tempOfOutputOfEnhanceLayer)\n",
    "\n",
    "    OutputOfEnhanceLayer = tansig(tempOfOutputOfEnhanceLayer * parameterOfShrink)\n",
    "    \n",
    "    #生成最终输入\n",
    "    InputOfOutputLayer = np.hstack([OutputOfFeatureMappingLayer,OutputOfEnhanceLayer])\n",
    "    pinvOfInput = pinv(InputOfOutputLayer,c)\n",
    "    OutputWeight = np.dot(pinvOfInput,train_y) \n",
    "    time_end=time.time() \n",
    "    trainTime = time_end - time_start\n",
    "    \n",
    "    OutputOfTrain = np.dot(InputOfOutputLayer,OutputWeight)\n",
    "    trainAcc = show_accuracy(OutputOfTrain,train_y)\n",
    "    print('Training accurate is' ,trainAcc*100,'%')\n",
    "    print('Training time is ',trainTime,'s')\n",
    "    train_acc_all[0][0] = trainAcc\n",
    "    train_time[0][0] = trainTime\n",
    "    #测试过程\n",
    "    test_x = preprocessing.scale(test_x,axis = 1)\n",
    "    FeatureOfInputDataWithBiasTest = np.hstack([test_x, 0.1 * np.ones((test_x.shape[0],1))])\n",
    "    OutputOfFeatureMappingLayerTest = np.zeros([test_x.shape[0],N2*N1])\n",
    "    time_start=time.time()\n",
    "\n",
    "    for i in range(N2):\n",
    "        outputOfEachWindowTest = np.dot(FeatureOfInputDataWithBiasTest,Beta1OfEachWindow[i])\n",
    "        OutputOfFeatureMappingLayerTest[:,N1*i:N1*(i+1)] =(ymax-ymin)*(outputOfEachWindowTest-minOfEachWindow[i])/distOfMaxAndMin[i]-ymin\n",
    "\n",
    "    InputOfEnhanceLayerWithBiasTest = np.hstack([OutputOfFeatureMappingLayerTest, 0.1 * np.ones((OutputOfFeatureMappingLayerTest.shape[0],1))])\n",
    "    tempOfOutputOfEnhanceLayerTest = np.dot(InputOfEnhanceLayerWithBiasTest,weightOfEnhanceLayer)\n",
    "\n",
    "    OutputOfEnhanceLayerTest = tansig(tempOfOutputOfEnhanceLayerTest * parameterOfShrink)    \n",
    "\n",
    "    InputOfOutputLayerTest = np.hstack([OutputOfFeatureMappingLayerTest,OutputOfEnhanceLayerTest])\n",
    "\n",
    "    OutputOfTest = np.dot(InputOfOutputLayerTest,OutputWeight)\n",
    "    time_end=time.time()\n",
    "    testTime = time_end - time_start\n",
    "    testAcc = show_accuracy(OutputOfTest,test_y)\n",
    "    print('Testing accurate is' ,testAcc * 100,'%')\n",
    "    print('Testing time is ',testTime,'s')\n",
    "    test_acc[0][0] = testAcc\n",
    "    test_time[0][0] = testTime\n",
    "\n",
    "    return test_acc,test_time,train_acc_all,train_time\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%    \n",
    "'''\n",
    "增加强化层节点版---BLS\n",
    "\n",
    "参数列表：\n",
    "s------收敛系数\n",
    "c------正则化系数\n",
    "N1-----映射层每个窗口内节点数\n",
    "N2-----映射层窗口数\n",
    "N3-----强化层节点数\n",
    "l------步数\n",
    "M------步长\n",
    "'''\n",
    "\n",
    "\n",
    "def BLS_AddEnhanceNodes(train_x,train_y,test_x,test_y,s,c,N1,N2,N3,L,M):\n",
    "    #生成映射层\n",
    "    '''\n",
    "    两个参数最重要，1）y;2)Beta1OfEachWindow\n",
    "    '''\n",
    "    u = 0\n",
    "\n",
    "    train_x = preprocessing.scale(train_x,axis = 1) #处理数据 \n",
    "    FeatureOfInputDataWithBias = np.hstack([train_x, 0.1 * np.ones((train_x.shape[0],1))])\n",
    "    OutputOfFeatureMappingLayer = np.zeros([train_x.shape[0],N2*N1])\n",
    "\n",
    "    distOfMaxAndMin = []\n",
    "    minOfEachWindow = []\n",
    "    train_acc = np.zeros([1,L+1])\n",
    "    test_acc = np.zeros([1,L+1])\n",
    "    train_time = np.zeros([1,L+1])\n",
    "    test_time = np.zeros([1,L+1])\n",
    "    time_start=time.time()#计时开始\n",
    "    Beta1OfEachWindow = []\n",
    "    for i in range(N2):\n",
    "        random.seed(i+u)\n",
    "        weightOfEachWindow = 2 * random.randn(train_x.shape[1]+1,N1)-1\n",
    "        FeatureOfEachWindow = np.dot(FeatureOfInputDataWithBias,weightOfEachWindow) \n",
    "        scaler1 = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(FeatureOfEachWindow)\n",
    "        FeatureOfEachWindowAfterPreprocess = scaler1.transform(FeatureOfEachWindow)\n",
    "        betaOfEachWindow  =  sparse_bls(FeatureOfEachWindowAfterPreprocess,FeatureOfInputDataWithBias).T\n",
    "        Beta1OfEachWindow.append(betaOfEachWindow)\n",
    "        outputOfEachWindow = np.dot(FeatureOfInputDataWithBias,betaOfEachWindow)\n",
    "        distOfMaxAndMin.append( np.max(outputOfEachWindow,axis =0) - np.min(outputOfEachWindow,axis =0))\n",
    "        minOfEachWindow.append(np.min(outputOfEachWindow,axis =0))\n",
    "        outputOfEachWindow = (outputOfEachWindow-minOfEachWindow[i])/distOfMaxAndMin[i]\n",
    "        OutputOfFeatureMappingLayer[:,N1*i:N1*(i+1)] = outputOfEachWindow\n",
    "        del outputOfEachWindow \n",
    "        del FeatureOfEachWindow \n",
    "        del weightOfEachWindow \n",
    "        \n",
    " \n",
    "    InputOfEnhanceLayerWithBias = np.hstack([OutputOfFeatureMappingLayer, 0.1 * np.ones((OutputOfFeatureMappingLayer.shape[0],1))])\n",
    "    if N1*N2>=N3:\n",
    "        random.seed(67797325)\n",
    "        weightOfEnhanceLayer = LA.orth(2 * random.randn(N2*N1+1,N3)-1)\n",
    "    else:\n",
    "        random.seed(67797325)\n",
    "        weightOfEnhanceLayer = LA.orth(2 * random.randn(N2*N1+1,N3).T-1).T\n",
    "    \n",
    "    tempOfOutputOfEnhanceLayer = np.dot(InputOfEnhanceLayerWithBias,weightOfEnhanceLayer)\n",
    "    parameterOfShrink = s/np.max(tempOfOutputOfEnhanceLayer)\n",
    "    OutputOfEnhanceLayer = tansig(tempOfOutputOfEnhanceLayer * parameterOfShrink)\n",
    "    \n",
    "    #生成最终输入\n",
    "    InputOfOutputLayer = np.hstack([OutputOfFeatureMappingLayer,OutputOfEnhanceLayer])\n",
    "    pinvOfInput = pinv(InputOfOutputLayer,c)\n",
    "    OutputWeight = pinvOfInput.dot(train_y) \n",
    "    time_end=time.time() \n",
    "    trainTime = time_end - time_start\n",
    "    \n",
    "    \n",
    "    OutputOfTrain = np.dot(InputOfOutputLayer,OutputWeight)\n",
    "    trainAcc = show_accuracy(OutputOfTrain,train_y)\n",
    "    print('Training accurate is' ,trainAcc*100,'%')\n",
    "    print('Training time is ',trainTime,'s')\n",
    "    train_acc[0][0] = trainAcc\n",
    "    train_time[0][0] = trainTime\n",
    "    \n",
    "    test_x = preprocessing.scale(test_x, axis=1) \n",
    "    FeatureOfInputDataWithBiasTest = np.hstack([test_x, 0.1 * np.ones((test_x.shape[0],1))])\n",
    "    OutputOfFeatureMappingLayerTest = np.zeros([test_x.shape[0],N2*N1])\n",
    "    time_start=time.time()\n",
    "\n",
    "    for i in range(N2):\n",
    "        outputOfEachWindowTest = np.dot(FeatureOfInputDataWithBiasTest,Beta1OfEachWindow[i])\n",
    "        OutputOfFeatureMappingLayerTest[:,N1*i:N1*(i+1)] = (outputOfEachWindowTest-minOfEachWindow[i])/distOfMaxAndMin[i]\n",
    "\n",
    "    InputOfEnhanceLayerWithBiasTest = np.hstack([OutputOfFeatureMappingLayerTest, 0.1 * np.ones((OutputOfFeatureMappingLayerTest.shape[0],1))])\n",
    "    tempOfOutputOfEnhanceLayerTest = np.dot(InputOfEnhanceLayerWithBiasTest,weightOfEnhanceLayer)\n",
    "\n",
    "    OutputOfEnhanceLayerTest = tansig(tempOfOutputOfEnhanceLayerTest * parameterOfShrink)    \n",
    "\n",
    "    InputOfOutputLayerTest = np.hstack([OutputOfFeatureMappingLayerTest,OutputOfEnhanceLayerTest])\n",
    " \n",
    "    OutputOfTest = np.dot(InputOfOutputLayerTest,OutputWeight)\n",
    "    time_end=time.time() #训练完成\n",
    "    testTime = time_end - time_start\n",
    "    testAcc = show_accuracy(OutputOfTest,test_y)\n",
    "    print('Testing accurate is' ,testAcc*100,'%')\n",
    "    print('Testing time is ',testTime,'s')\n",
    "    test_acc[0][0] = testAcc\n",
    "    test_time[0][0] = testTime\n",
    "    '''\n",
    "        增量增加强化节点\n",
    "    '''\n",
    "    parameterOfShrinkAdd = []\n",
    "    for e in list(range(L)):\n",
    "        time_start=time.time()\n",
    "        if N1*N2>= M : \n",
    "            random.seed(e)\n",
    "            weightOfEnhanceLayerAdd = LA.orth(2 * random.randn(N2*N1+1,M)-1)\n",
    "        else :\n",
    "            random.seed(e)\n",
    "            weightOfEnhanceLayerAdd = LA.orth(2 * random.randn(N2*N1+1,M).T-1).T\n",
    "        \n",
    "        tempOfOutputOfEnhanceLayerAdd = np.dot(InputOfEnhanceLayerWithBias,weightOfEnhanceLayerAdd)\n",
    "        parameterOfShrinkAdd.append(s/np.max(tempOfOutputOfEnhanceLayerAdd))\n",
    "        OutputOfEnhanceLayerAdd = tansig(tempOfOutputOfEnhanceLayerAdd*parameterOfShrinkAdd[e])\n",
    "        tempOfLastLayerInput = np.hstack([InputOfOutputLayer,OutputOfEnhanceLayerAdd])\n",
    "        \n",
    "        D = pinvOfInput.dot(OutputOfEnhanceLayerAdd)\n",
    "        C = OutputOfEnhanceLayerAdd - InputOfOutputLayer.dot(D)\n",
    "        if C.all() == 0:\n",
    "            w = D.shape[1]\n",
    "            B = np.mat(np.eye(w) - np.dot(D.T,D)).I.dot(np.dot(D.T,pinvOfInput))\n",
    "        else:\n",
    "            B = pinv(C,c)\n",
    "        pinvOfInput = np.vstack([(pinvOfInput - D.dot(B)),B])\n",
    "        OutputWeightEnd = pinvOfInput.dot(train_y)\n",
    "        InputOfOutputLayer = tempOfLastLayerInput\n",
    "        Training_time = time.time() - time_start\n",
    "        train_time[0][e+1] = Training_time\n",
    "        OutputOfTrain1 = InputOfOutputLayer.dot(OutputWeightEnd)\n",
    "        TrainingAccuracy = show_accuracy(OutputOfTrain1,train_y)\n",
    "        train_acc[0][e+1] = TrainingAccuracy\n",
    "        print('Incremental Training Accuracy is :', TrainingAccuracy * 100, ' %' )\n",
    "        \n",
    "\n",
    "        time_start = time.time()\n",
    "        OutputOfEnhanceLayerAddTest = tansig(InputOfEnhanceLayerWithBiasTest.dot(weightOfEnhanceLayerAdd) * parameterOfShrinkAdd[e])\n",
    "        InputOfOutputLayerTest=np.hstack([InputOfOutputLayerTest, OutputOfEnhanceLayerAddTest])\n",
    "\n",
    "        OutputOfTest1 = InputOfOutputLayerTest.dot(OutputWeightEnd)\n",
    "        TestingAcc = show_accuracy(OutputOfTest1,test_y)\n",
    "        \n",
    "        Test_time = time.time() - time_start\n",
    "        test_time[0][e+1] = Test_time\n",
    "        test_acc[0][e+1] = TestingAcc\n",
    "        print('Incremental Testing Accuracy is : ', TestingAcc * 100, ' %' )\n",
    "        \n",
    "    return test_acc,test_time,train_acc,train_time\n",
    "\n",
    "\n",
    "'''\n",
    "增加强化层节点版---BLS\n",
    "\n",
    "参数列表：\n",
    "s------收敛系数\n",
    "c------正则化系数\n",
    "N1-----映射层每个窗口内节点数\n",
    "N2-----映射层窗口数\n",
    "N3-----强化层节点数\n",
    "L------步数\n",
    "\n",
    "M1-----增加映射节点数\n",
    "M2-----与增加映射节点对应的强化节点数\n",
    "M3-----新增加的强化节点\n",
    "'''\n",
    "#%%%%%%%%%%%%%%%%\n",
    "def BLS_AddFeatureEnhanceNodes(train_x,train_y,test_x,test_y,s,c,N1,N2,N3,L,M1,M2,M3):\n",
    "    \n",
    "    #生成映射层\n",
    "    '''\n",
    "    两个参数最重要，1）y;2)Beta1OfEachWindow\n",
    "    '''\n",
    "    u = 0\n",
    "\n",
    "    train_x = preprocessing.scale(train_x,axis = 1) \n",
    "    FeatureOfInputDataWithBias = np.hstack([train_x, 0.1 * np.ones((train_x.shape[0],1))])\n",
    "    OutputOfFeatureMappingLayer = np.zeros([train_x.shape[0],N2*N1])\n",
    "\n",
    "    Beta1OfEachWindow = list()\n",
    "    distOfMaxAndMin = []\n",
    "    minOfEachWindow = []\n",
    "    train_acc = np.zeros([1,L+1])\n",
    "    test_acc = np.zeros([1,L+1])\n",
    "    train_time = np.zeros([1,L+1])\n",
    "    test_time = np.zeros([1,L+1])\n",
    "    time_start=time.time()#计时开始\n",
    "    for i in range(N2):\n",
    "        random.seed(i+u)\n",
    "        weightOfEachWindow = 2 * random.randn(train_x.shape[1]+1,N1)-1\n",
    "        FeatureOfEachWindow = np.dot(FeatureOfInputDataWithBias,weightOfEachWindow) \n",
    "        scaler1 = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(FeatureOfEachWindow)\n",
    "        FeatureOfEachWindowAfterPreprocess = scaler1.transform(FeatureOfEachWindow)\n",
    "        betaOfEachWindow  =  sparse_bls(FeatureOfEachWindowAfterPreprocess,FeatureOfInputDataWithBias).T\n",
    "        Beta1OfEachWindow.append(betaOfEachWindow)\n",
    "        outputOfEachWindow = np.dot(FeatureOfInputDataWithBias,betaOfEachWindow)\n",
    "        distOfMaxAndMin.append(np.max(outputOfEachWindow,axis = 0) - np.min(outputOfEachWindow,axis = 0))\n",
    "        minOfEachWindow.append(np.mean(outputOfEachWindow,axis = 0))\n",
    "        outputOfEachWindow = (outputOfEachWindow-minOfEachWindow[i])/distOfMaxAndMin[i]\n",
    "        OutputOfFeatureMappingLayer[:,N1*i:N1*(i+1)] = outputOfEachWindow\n",
    "        del outputOfEachWindow \n",
    "        del FeatureOfEachWindow \n",
    "        del weightOfEachWindow \n",
    "        \n",
    "    #生成强化层\n",
    " \n",
    "    InputOfEnhanceLayerWithBias = np.hstack([OutputOfFeatureMappingLayer, 0.1 * np.ones((OutputOfFeatureMappingLayer.shape[0],1))])\n",
    "\n",
    "    if N1*N2>=N3:\n",
    "        random.seed(67797325)\n",
    "        weightOfEnhanceLayer = LA.orth(2 * random.randn(N2*N1+1,N3)-1)\n",
    "    else:\n",
    "        random.seed(67797325)\n",
    "        weightOfEnhanceLayer = LA.orth(2 * random.randn(N2*N1+1,N3).T-1).T\n",
    "    \n",
    "    tempOfOutputOfEnhanceLayer = np.dot(InputOfEnhanceLayerWithBias,weightOfEnhanceLayer)\n",
    "    parameterOfShrink = s/np.max(tempOfOutputOfEnhanceLayer)\n",
    "    OutputOfEnhanceLayer = tansig(tempOfOutputOfEnhanceLayer * parameterOfShrink)\n",
    "    \n",
    "    #生成最终输入\n",
    "    InputOfOutputLayerTrain = np.hstack([OutputOfFeatureMappingLayer,OutputOfEnhanceLayer])\n",
    "    pinvOfInput = pinv(InputOfOutputLayerTrain,c)\n",
    "    OutputWeight =pinvOfInput.dot(train_y) #全局违逆\n",
    "    time_end=time.time() #训练完成\n",
    "    trainTime = time_end - time_start\n",
    "    \n",
    "    OutputOfTrain = np.dot(InputOfOutputLayerTrain,OutputWeight)\n",
    "    trainAcc = show_accuracy(OutputOfTrain,train_y)\n",
    "    print('Training accurate is' ,trainAcc*100,'%')\n",
    "    print('Training time is ',trainTime,'s')\n",
    "    train_acc[0][0] = trainAcc\n",
    "    train_time[0][0] = trainTime\n",
    "\n",
    "    test_x = preprocessing.scale(test_x,axis = 1) \n",
    "    FeatureOfInputDataWithBiasTest = np.hstack([test_x, 0.1 * np.ones((test_x.shape[0],1))])\n",
    "    OutputOfFeatureMappingLayerTest = np.zeros([test_x.shape[0],N2*N1])\n",
    "    time_start=time.time()\n",
    "\n",
    "    for i in range(N2):\n",
    "        outputOfEachWindowTest = np.dot(FeatureOfInputDataWithBiasTest,Beta1OfEachWindow[i])\n",
    "        OutputOfFeatureMappingLayerTest[:,N1*i:N1*(i+1)] = (outputOfEachWindowTest-minOfEachWindow[i])/distOfMaxAndMin[i] \n",
    "\n",
    "    InputOfEnhanceLayerWithBiasTest = np.hstack([OutputOfFeatureMappingLayerTest, 0.1 * np.ones((OutputOfFeatureMappingLayerTest.shape[0],1))])\n",
    "    tempOfOutputOfEnhanceLayerTest = np.dot(InputOfEnhanceLayerWithBiasTest,weightOfEnhanceLayer)\n",
    "\n",
    "    OutputOfEnhanceLayerTest = tansig(tempOfOutputOfEnhanceLayerTest * parameterOfShrink)    \n",
    "\n",
    "    InputOfOutputLayerTest = np.hstack([OutputOfFeatureMappingLayerTest,OutputOfEnhanceLayerTest])\n",
    "  \n",
    "    OutputOfTest = np.dot(InputOfOutputLayerTest,OutputWeight)\n",
    "    time_end=time.time() \n",
    "    testTime = time_end - time_start\n",
    "    testAcc = show_accuracy(OutputOfTest,test_y)\n",
    "    print('Testing accurate is' ,testAcc*100,'%')\n",
    "    print('Testing time is ',testTime,'s')\n",
    "    test_acc[0][0] = testAcc\n",
    "    test_time[0][0] = testTime\n",
    "    '''\n",
    "        增加Mapping 和 强化节点\n",
    "    '''\n",
    "    WeightOfNewFeature2 = list()\n",
    "    WeightOfNewFeature3 = list()\n",
    "    for e in list(range(L)):\n",
    "        time_start = time.time()\n",
    "        random.seed(e+N2+u)\n",
    "        weightOfNewMapping = 2 * random.random([train_x.shape[1]+1,M1]) - 1\n",
    "        NewMappingOutput = FeatureOfInputDataWithBias.dot(weightOfNewMapping)\n",
    "\n",
    "        scaler2 = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(NewMappingOutput)\n",
    "        FeatureOfEachWindowAfterPreprocess = scaler2.transform(NewMappingOutput)\n",
    "        betaOfNewWindow  =  sparse_bls(FeatureOfEachWindowAfterPreprocess,FeatureOfInputDataWithBias).T\n",
    "        Beta1OfEachWindow.append(betaOfNewWindow)\n",
    "   \n",
    "        TempOfFeatureOutput = FeatureOfInputDataWithBias.dot(betaOfNewWindow)\n",
    "        distOfMaxAndMin.append( np.max(TempOfFeatureOutput,axis = 0) - np.min(TempOfFeatureOutput,axis = 0))\n",
    "        minOfEachWindow.append(np.mean(TempOfFeatureOutput,axis = 0))\n",
    "        outputOfNewWindow = (TempOfFeatureOutput-minOfEachWindow[N2+e])/distOfMaxAndMin[N2+e]\n",
    "\n",
    "        OutputOfFeatureMappingLayer = np.hstack([OutputOfFeatureMappingLayer,outputOfNewWindow])\n",
    "\n",
    "        NewInputOfEnhanceLayerWithBias = np.hstack([outputOfNewWindow, 0.1 * np.ones((outputOfNewWindow.shape[0],1))])\n",
    "\n",
    "        if M1 >= M2:\n",
    "            random.seed(67797325)\n",
    "            RelateEnhanceWeightOfNewFeatureNodes = LA.orth(2*random.random([M1+1,M2])-1)\n",
    "        else:\n",
    "            random.seed(67797325)\n",
    "            RelateEnhanceWeightOfNewFeatureNodes = LA.orth(2*random.random([M1+1,M2]).T-1).T  \n",
    "        WeightOfNewFeature2.append(RelateEnhanceWeightOfNewFeatureNodes)\n",
    "        \n",
    "        tempOfNewFeatureEhanceNodes = NewInputOfEnhanceLayerWithBias.dot(RelateEnhanceWeightOfNewFeatureNodes)\n",
    "        \n",
    "        parameter1 = s/np.max(tempOfNewFeatureEhanceNodes)\n",
    "\n",
    "        outputOfNewFeatureEhanceNodes = tansig(tempOfNewFeatureEhanceNodes * parameter1)\n",
    "\n",
    "        if N2*N1+e*M1>=M3:\n",
    "            random.seed(67797325+e)\n",
    "            weightOfNewEnhanceNodes = LA.orth(2 * random.randn(N2*N1+(e+1)*M1+1,M3) - 1)\n",
    "        else:\n",
    "            random.seed(67797325+e)\n",
    "            weightOfNewEnhanceNodes = LA.orth(2 * random.randn(N2*N1+(e+1)*M1+1,M3).T-1).T\n",
    "        WeightOfNewFeature3.append(weightOfNewEnhanceNodes)\n",
    "\n",
    "        InputOfEnhanceLayerWithBias = np.hstack([OutputOfFeatureMappingLayer, 0.1 * np.ones((OutputOfFeatureMappingLayer.shape[0],1))])\n",
    "\n",
    "        tempOfNewEnhanceNodes = InputOfEnhanceLayerWithBias.dot(weightOfNewEnhanceNodes)\n",
    "        parameter2 = s/np.max(tempOfNewEnhanceNodes)\n",
    "        OutputOfNewEnhanceNodes = tansig(tempOfNewEnhanceNodes * parameter2)\n",
    "        OutputOfTotalNewAddNodes = np.hstack([outputOfNewWindow,outputOfNewFeatureEhanceNodes,OutputOfNewEnhanceNodes])\n",
    "        tempOfInputOfLastLayes = np.hstack([InputOfOutputLayerTrain,OutputOfTotalNewAddNodes])\n",
    "        D = pinvOfInput.dot(OutputOfTotalNewAddNodes)\n",
    "        C = OutputOfTotalNewAddNodes - InputOfOutputLayerTrain.dot(D)\n",
    "        \n",
    "        if C.all() == 0:\n",
    "            w = D.shape[1]\n",
    "            B = (np.eye(w)- D.T.dot(D)).I.dot(D.T.dot(pinvOfInput))\n",
    "        else:\n",
    "            B = pinv(C,c)\n",
    "        pinvOfInput = np.vstack([(pinvOfInput - D.dot(B)),B])\n",
    "        OutputWeight = pinvOfInput.dot(train_y)        \n",
    "        InputOfOutputLayerTrain = tempOfInputOfLastLayes\n",
    "        \n",
    "        time_end = time.time()\n",
    "        Train_time = time_end - time_start\n",
    "        train_time[0][e+1] = Train_time\n",
    "        predictLabel = InputOfOutputLayerTrain.dot(OutputWeight)\n",
    "        TrainingAccuracy = show_accuracy(predictLabel,train_y)\n",
    "        train_acc[0][e+1] = TrainingAccuracy\n",
    "        print('Incremental Training Accuracy is :', TrainingAccuracy * 100, ' %' )\n",
    "        \n",
    "        # 测试过程\n",
    "        #先生成新映射窗口输出\n",
    "        time_start = time.time() \n",
    "        WeightOfNewMapping =  Beta1OfEachWindow[N2+e]\n",
    "\n",
    "        outputOfNewWindowTest = FeatureOfInputDataWithBiasTest.dot(WeightOfNewMapping )\n",
    "        \n",
    "        outputOfNewWindowTest = (outputOfNewWindowTest-minOfEachWindow[N2+e])/distOfMaxAndMin[N2+e] \n",
    "        \n",
    "        OutputOfFeatureMappingLayerTest = np.hstack([OutputOfFeatureMappingLayerTest,outputOfNewWindowTest])\n",
    "        \n",
    "        InputOfEnhanceLayerWithBiasTest = np.hstack([OutputOfFeatureMappingLayerTest,0.1*np.ones([OutputOfFeatureMappingLayerTest.shape[0],1])])\n",
    "        \n",
    "        NewInputOfEnhanceLayerWithBiasTest = np.hstack([outputOfNewWindowTest,0.1*np.ones([outputOfNewWindowTest.shape[0],1])])\n",
    "\n",
    "        weightOfRelateNewEnhanceNodes = WeightOfNewFeature2[e]\n",
    "        \n",
    "        OutputOfRelateEnhanceNodes = tansig(NewInputOfEnhanceLayerWithBiasTest.dot(weightOfRelateNewEnhanceNodes) * parameter1)\n",
    "        \n",
    "        weightOfNewEnhanceNodes = WeightOfNewFeature3[e]\n",
    "        \n",
    "        OutputOfNewEnhanceNodes = tansig(InputOfEnhanceLayerWithBiasTest.dot(weightOfNewEnhanceNodes)*parameter2)\n",
    "        \n",
    "        InputOfOutputLayerTest = np.hstack([InputOfOutputLayerTest,outputOfNewWindowTest,OutputOfRelateEnhanceNodes,OutputOfNewEnhanceNodes])\n",
    "    \n",
    "        predictLabel = InputOfOutputLayerTest.dot(OutputWeight)\n",
    "\n",
    "        TestingAccuracy = show_accuracy(predictLabel,test_y)\n",
    "        time_end = time.time()\n",
    "        Testing_time= time_end - time_start\n",
    "        test_time[0][e+1] = Testing_time\n",
    "        test_acc[0][e+1]=TestingAccuracy\n",
    "        print('Testing Accuracy is : ', TestingAccuracy * 100, ' %' )\n",
    "\n",
    "    return test_acc,test_time,train_acc,train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b528241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74789b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a2fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
